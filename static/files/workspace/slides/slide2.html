<!DOCTYPE html>
<html>
<head>
<style>
html { background: #ffffff; }
body {
  width: 720pt; height: 405pt; margin: 0; padding: 0;
  background: #ffffff; font-family: Arial, sans-serif;
  display: flex;
}
.container {
  margin: 24pt;
  width: 672pt;
  height: 357pt;
  background: #F8F9FA;
  border-radius: 12pt;
  position: relative;
}
.accent-bar {
  position: absolute;
  left: 0; top: 0;
  width: 8pt; height: 100%;
  background: #16A085;
  border-radius: 12pt 0 0 12pt;
}
.header-badge {
  position: absolute;
  top: 28pt; left: 40pt;
  background: #1C2833;
  padding: 6pt 16pt;
  border-radius: 4pt;
}
.header-badge p {
  margin: 0;
  color: #ffffff;
  font-size: 10pt;
  font-weight: bold;
  letter-spacing: 1pt;
}
.section-tag {
  position: absolute;
  top: 28pt; right: 40pt;
  background: #E74C3C;
  padding: 6pt 14pt;
  border-radius: 4pt;
}
.section-tag p {
  margin: 0;
  color: #ffffff;
  font-size: 10pt;
  font-weight: bold;
}
.title {
  position: absolute;
  top: 75pt; left: 40pt;
}
.title h1 {
  margin: 0;
  font-family: Georgia, serif;
  font-size: 26pt;
  font-weight: bold;
  color: #1C2833;
}
.cards {
  position: absolute;
  top: 125pt; left: 40pt; right: 40pt;
  display: flex;
  gap: 20pt;
}
.card {
  flex: 1;
  background: #ffffff;
  border-radius: 10pt;
  padding: 20pt;
  box-shadow: 0 2pt 8pt rgba(0,0,0,0.06);
  border-left: 4pt solid #16A085;
}
.card h2 {
  margin: 0 0 10pt 0;
  font-size: 14pt;
  font-weight: bold;
  color: #1C2833;
}
.card p {
  margin: 0;
  font-size: 11pt;
  color: #5D6D7E;
  line-height: 1.5;
}
</style>
</head>
<body>
<div class="container">
  <div class="accent-bar"></div>
  <div class="header-badge"><p>TRANSFORMER INTERNALS</p></div>
  <div class="section-tag"><p>WHAT CHANGED</p></div>
  <div class="title">
    <h1>Three upgrades that unlocked scale</h1>
  </div>
  <div class="cards">
    <div class="card">
      <h2>Position Encoding</h2>
      <p>Learned vectors gave way to RoPE/ALiBi, injecting order where attention happens.</p>
    </div>
    <div class="card">
      <h2>Normalization</h2>
      <p>Post-norm became pre-norm, and LayerNorm simplified into faster RMSNorm.</p>
    </div>
    <div class="card">
      <h2>Attention Scaling</h2>
      <p>Full attention morphed into sparse windows and GQA to control O(n^2) costs.</p>
    </div>
  </div>
</div>
</body>
</html>
