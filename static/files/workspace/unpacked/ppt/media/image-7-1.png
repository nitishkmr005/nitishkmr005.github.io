<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1200 800" style="font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;">
  <defs>
    <linearGradient id="ropeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#EF4444;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#B91C1C;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="nopeGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#6B7280;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#374151;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="linearGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#06B6D4;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#0891B2;stop-opacity:1" />
    </linearGradient>
    <linearGradient id="hybridGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#8B5CF6;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#6D28D9;stop-opacity:1" />
    </linearGradient>
    <filter id="shadow" x="-20%" y="-20%" width="140%" height="140%">
      <feDropShadow dx="2" dy="3" stdDeviation="3" flood-opacity="0.15"/>
    </filter>
  </defs>
  
  <!-- Background -->
  <rect width="1200" height="800" fill="#F8FAFC"/>
  
  <!-- Title -->
  <text x="600" y="38" text-anchor="middle" font-size="28" font-weight="bold" fill="#1E293B">
    Advanced Concepts: NoPE &amp; Linear Attention Hybrids
  </text>
  <text x="600" y="62" text-anchor="middle" font-size="13" fill="#64748B">
    Emerging techniques for better length generalization and computational efficiency
  </text>
  
  <!-- NoPE Section -->
  <g transform="translate(40, 90)">
    <rect x="0" y="0" width="540" height="300" rx="12" fill="white" filter="url(#shadow)"/>
    <rect x="0" y="0" width="540" height="45" rx="12" fill="url(#nopeGrad)"/>
    <rect x="0" y="32" width="540" height="13" fill="url(#nopeGrad)"/>
    <text x="270" y="30" text-anchor="middle" font-size="16" font-weight="bold" fill="white">
      NoPE: No Positional Embedding
    </text>
    <text x="270" y="65" text-anchor="middle" font-size="11" fill="#6B7280">
      Used in: SmolLM3 (3/4 layers), Kimi Linear (MLA layers)
    </text>
    
    <!-- Side by side comparison -->
    <g transform="translate(25, 85)">
      <!-- RoPE -->
      <rect x="0" y="0" width="220" height="140" rx="8" fill="#FEF2F2" stroke="#EF4444" stroke-width="2"/>
      <text x="110" y="22" text-anchor="middle" font-size="12" font-weight="bold" fill="#B91C1C">Standard RoPE</text>
      
      <rect x="30" y="38" width="160" height="22" rx="4" fill="#FECACA"/>
      <text x="110" y="54" text-anchor="middle" font-size="9" fill="#7F1D1D">Token Embeddings</text>
      
      <text x="110" y="72" text-anchor="middle" font-size="14" fill="#EF4444">‚Üì</text>
      
      <rect x="30" y="78" width="160" height="25" rx="4" fill="url(#ropeGrad)"/>
      <text x="110" y="95" text-anchor="middle" font-size="9" fill="white" font-weight="bold">Rotate: q'=q¬∑R(Œ∏¬∑pos)</text>
      
      <text x="110" y="118" text-anchor="middle" font-size="14" fill="#EF4444">‚Üì</text>
      <text x="110" y="135" text-anchor="middle" font-size="10" fill="#1D4ED8">Attention</text>
      
      <!-- NoPE -->
      <rect x="250" y="0" width="220" height="140" rx="8" fill="#F3F4F6" stroke="#6B7280" stroke-width="2"/>
      <text x="360" y="22" text-anchor="middle" font-size="12" font-weight="bold" fill="#374151">NoPE Layers</text>
      
      <rect x="280" y="38" width="160" height="22" rx="4" fill="#E5E7EB"/>
      <text x="360" y="54" text-anchor="middle" font-size="9" fill="#374151">Token Embeddings</text>
      
      <text x="360" y="72" text-anchor="middle" font-size="14" fill="#6B7280">‚Üì</text>
      <text x="430" y="72" text-anchor="middle" font-size="8" fill="#9CA3AF" font-style="italic">(skip!)</text>
      
      <rect x="280" y="78" width="160" height="25" rx="4" fill="#D1D5DB"/>
      <text x="360" y="95" text-anchor="middle" font-size="9" fill="#374151">Causal Mask Only</text>
      
      <text x="360" y="118" text-anchor="middle" font-size="14" fill="#6B7280">‚Üì</text>
      <text x="360" y="135" text-anchor="middle" font-size="10" fill="#1D4ED8">Attention</text>
    </g>
    
    <!-- Key insight -->
    <g transform="translate(25, 235)">
      <rect x="0" y="0" width="490" height="50" rx="6" fill="#ECFDF5"/>
      <text x="15" y="20" font-size="11" font-weight="bold" fill="#047857">üí° Key Insight:</text>
      <text x="15" y="38" font-size="10" fill="#475569">Causal mask implicitly encodes order ‚Üí model learns position from context!</text>
      <text x="400" y="38" font-size="10" fill="#10B981" font-weight="bold">Better length gen!</text>
    </g>
  </g>
  
  <!-- Linear Attention Section -->
  <g transform="translate(620, 90)">
    <rect x="0" y="0" width="540" height="300" rx="12" fill="white" filter="url(#shadow)"/>
    <rect x="0" y="0" width="540" height="45" rx="12" fill="url(#linearGrad)"/>
    <rect x="0" y="32" width="540" height="13" fill="url(#linearGrad)"/>
    <text x="270" y="30" text-anchor="middle" font-size="16" font-weight="bold" fill="white">
      Linear Attention &amp; Gated DeltaNet
    </text>
    <text x="270" y="65" text-anchor="middle" font-size="11" fill="#0891B2">
      Revival: Qwen3-Next, Kimi Linear, MiniMax-M1
    </text>
    
    <!-- Complexity comparison -->
    <g transform="translate(25, 85)">
      <rect x="0" y="0" width="220" height="100" rx="6" fill="#FEF3C7" stroke="#F59E0B" stroke-width="2"/>
      <text x="110" y="22" text-anchor="middle" font-size="11" font-weight="bold" fill="#92400E">Standard Attention</text>
      <text x="110" y="42" text-anchor="middle" font-size="10" fill="#475569">softmax(QK^T) ¬∑ V</text>
      <text x="110" y="65" text-anchor="middle" font-size="14" fill="#DC2626" font-weight="bold">O(n¬≤)</text>
      <text x="110" y="85" text-anchor="middle" font-size="9" fill="#94A3B8">Full n√ón matrix</text>
      
      <rect x="250" y="0" width="220" height="100" rx="6" fill="#ECFDF5" stroke="#10B981" stroke-width="2"/>
      <text x="360" y="22" text-anchor="middle" font-size="11" font-weight="bold" fill="#047857">Linear Attention</text>
      <text x="360" y="42" text-anchor="middle" font-size="10" fill="#475569">œÜ(Q)¬∑(œÜ(K)^T¬∑V)</text>
      <text x="360" y="65" text-anchor="middle" font-size="14" fill="#10B981" font-weight="bold">O(n)</text>
      <text x="360" y="85" text-anchor="middle" font-size="9" fill="#94A3B8">Avoid n√ón!</text>
    </g>
    
    <!-- Gated DeltaNet -->
    <g transform="translate(25, 195)">
      <rect x="0" y="0" width="490" height="90" rx="6" fill="#F5F3FF" stroke="#8B5CF6" stroke-width="1"/>
      <text x="245" y="20" text-anchor="middle" font-size="11" font-weight="bold" fill="#6D28D9">Gated DeltaNet (Qwen3-Next, Kimi Linear)</text>
      <text x="15" y="42" font-size="10" fill="#475569">‚Ä¢ Linear attention + RNN-style gating (Œ±, Œ≤ gates)</text>
      <text x="15" y="58" font-size="10" fill="#475569">‚Ä¢ Maintains "fast-weight memory" for context</text>
      <text x="15" y="74" font-size="10" fill="#6D28D9" font-weight="bold">‚Ä¢ Kimi: channel-wise gating for better long-context reasoning</text>
    </g>
  </g>
  
  <!-- Hybrid Architecture Section -->
  <g transform="translate(40, 410)">
    <rect x="0" y="0" width="1120" height="370" rx="12" fill="white" filter="url(#shadow)"/>
    <rect x="0" y="0" width="1120" height="45" rx="12" fill="url(#hybridGrad)"/>
    <rect x="0" y="32" width="1120" height="13" fill="url(#hybridGrad)"/>
    <text x="560" y="30" text-anchor="middle" font-size="16" font-weight="bold" fill="white">
      üîÄ The 3:1 Hybrid Pattern: Linear + Full Attention
    </text>
    
    <!-- Timeline -->
    <g transform="translate(60, 65)">
      <line x1="0" y1="20" x2="980" y2="20" stroke="#CBD5E1" stroke-width="3"/>
      
      <!-- Models on timeline -->
      <g transform="translate(80, 0)">
        <circle cx="0" cy="20" r="10" fill="#F97316"/>
        <text x="0" y="50" text-anchor="middle" font-size="10" fill="#475569" font-weight="bold">MiniMax-M1</text>
        <text x="0" y="65" text-anchor="middle" font-size="8" fill="#F97316">Lightning Attn</text>
      </g>
      
      <g transform="translate(300, 0)">
        <circle cx="0" cy="20" r="10" fill="#EF4444"/>
        <text x="0" y="50" text-anchor="middle" font-size="10" fill="#475569" font-weight="bold">Qwen3-Next</text>
        <text x="0" y="65" text-anchor="middle" font-size="8" fill="#EF4444">Gated DeltaNet</text>
      </g>
      
      <g transform="translate(520, 0)">
        <circle cx="0" cy="20" r="10" fill="#8B5CF6"/>
        <text x="0" y="50" text-anchor="middle" font-size="10" fill="#475569" font-weight="bold">DeepSeek V3.2</text>
        <text x="0" y="65" text-anchor="middle" font-size="8" fill="#8B5CF6">Sparse Attn</text>
      </g>
      
      <g transform="translate(700, 0)">
        <circle cx="0" cy="20" r="10" fill="#6B7280"/>
        <text x="0" y="50" text-anchor="middle" font-size="10" fill="#475569" font-weight="bold">MiniMax-M2</text>
        <text x="0" y="65" text-anchor="middle" font-size="8" fill="#DC2626">Back to Full!</text>
      </g>
      
      <g transform="translate(880, 0)">
        <circle cx="0" cy="20" r="10" fill="#10B981"/>
        <text x="0" y="50" text-anchor="middle" font-size="10" fill="#475569" font-weight="bold">Kimi Linear</text>
        <text x="0" y="65" text-anchor="middle" font-size="8" fill="#10B981">KDA + MLA</text>
      </g>
    </g>
    
    <!-- Layer pattern visualization -->
    <g transform="translate(60, 160)">
      <text x="0" y="0" font-size="12" font-weight="bold" fill="#1E293B">3:1 Hybrid Pattern:</text>
      
      <g transform="translate(0, 15)">
        <rect x="0" y="0" width="110" height="40" rx="5" fill="url(#linearGrad)"/>
        <text x="55" y="18" text-anchor="middle" font-size="9" fill="white" font-weight="bold">Layer 1-3</text>
        <text x="55" y="32" text-anchor="middle" font-size="8" fill="#E0F2FE">DeltaNet</text>
        
        <rect x="120" y="0" width="110" height="40" rx="5" fill="url(#linearGrad)"/>
        <text x="175" y="18" text-anchor="middle" font-size="9" fill="white" font-weight="bold">Layer 2</text>
        <text x="175" y="32" text-anchor="middle" font-size="8" fill="#E0F2FE">DeltaNet</text>
        
        <rect x="240" y="0" width="110" height="40" rx="5" fill="url(#linearGrad)"/>
        <text x="295" y="18" text-anchor="middle" font-size="9" fill="white" font-weight="bold">Layer 3</text>
        <text x="295" y="32" text-anchor="middle" font-size="8" fill="#E0F2FE">DeltaNet</text>
        
        <rect x="360" y="0" width="110" height="40" rx="5" fill="#3B82F6" filter="url(#shadow)"/>
        <text x="415" y="18" text-anchor="middle" font-size="9" fill="white" font-weight="bold">Layer 4</text>
        <text x="415" y="32" text-anchor="middle" font-size="8" fill="#BFDBFE">Full Attn ‚òÖ</text>
        
        <text x="490" y="25" font-size="20" fill="#9CA3AF">‚Üí repeat</text>
      </g>
      
      <!-- Why boxes -->
      <g transform="translate(600, 0)">
        <rect x="0" y="0" width="360" height="55" rx="6" fill="#F0FDF4"/>
        <text x="180" y="18" text-anchor="middle" font-size="11" font-weight="bold" fill="#047857">Why 3:1 works:</text>
        <text x="15" y="35" font-size="9" fill="#475569">‚Ä¢ 75% linear = efficient for long sequences</text>
        <text x="15" y="48" font-size="9" fill="#475569">‚Ä¢ 25% full attention = maintains quality</text>
      </g>
    </g>
    
    <!-- M2 story -->
    <g transform="translate(60, 240)">
      <rect x="0" y="0" width="480" height="50" rx="6" fill="#FEF2F2"/>
      <text x="15" y="20" font-size="10" font-weight="bold" fill="#B91C1C">‚ö†Ô∏è MiniMax-M2 Plot Twist:</text>
      <text x="15" y="38" font-size="9" fill="#475569">Went back to full attention! Linear had issues with reasoning &amp; multi-turn tasks.</text>
    </g>
    
    <!-- Kimi Linear success -->
    <g transform="translate(560, 240)">
      <rect x="0" y="0" width="480" height="50" rx="6" fill="#ECFDF5"/>
      <text x="15" y="20" font-size="10" font-weight="bold" fill="#047857">‚úì Kimi Linear Success:</text>
      <text x="15" y="38" font-size="9" fill="#475569">Fixed with Kimi Delta Attention (channel-wise gating) + MLA in full layers!</text>
    </g>
    
    <!-- Bottom summary -->
    <g transform="translate(60, 305)">
      <rect x="0" y="0" width="1000" height="50" rx="8" fill="#1E293B"/>
      <text x="500" y="22" text-anchor="middle" font-size="12" font-weight="bold" fill="white">
        üí° 2025 Trend: Linear attention is viable again with right architecture (3:1 hybrid + improved gating)
      </text>
      <text x="500" y="40" text-anchor="middle" font-size="10" fill="#94A3B8">
        Trade-off: Efficiency vs Quality | Solution: Hybrid approach with periodic full attention
      </text>
    </g>
  </g>
</svg>
